# banditproblem
A study on Multi-armed bandit problem


Algorithms included:

- epislon greedy
- epislon first
- epislon decreasing
- UCB
